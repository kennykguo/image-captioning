{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5fa033-e1a8-4589-97b3-3076f81754c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479d8963-fd47-4d73-9973-ae4b563f0881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utils in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6764e3-afbd-459b-85b7-aa01d4aef667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a1d2ae-233c-4a56-86d8-513f5cd27c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b224c4-d347-485c-b40d-27a3b3c94c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a1df8e8-2155-4a87-bbf8-654bfe483674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"SENTENCE_START it's a slight ppr league- .2 ppr. SENTENCE_END\", 'SENTENCE_START standard besides 1 points for 15 yards receiving, .2 points per completion, 6 points per td thrown, and some bonuses for rec/rush/pass yardage. SENTENCE_END', 'SENTENCE_START my question is, is it wildly clear that qb has the highest potential for points? SENTENCE_END', 'SENTENCE_START i put in the rules at a ranking site and noticed that top qbs had 300 points more than the top rb/wr. SENTENCE_END', 'SENTENCE_START would it be dumb not to grab a qb in the first round? SENTENCE_END', 'SENTENCE_START in your scenario, a person could just not run the mandatory background check on the buyer and still sell the gun to the felon. SENTENCE_END', \"SENTENCE_START there's no way to enforce it. SENTENCE_END\", \"SENTENCE_START an honest seller is going to not sell the gun to them when they see they're a felon on the background check. SENTENCE_END\", \"SENTENCE_START a dishonest seller isn't going to run the check in the first place. SENTENCE_END\"]\n",
      "Parsed 79184 sentences.\n"
     ]
    }
   ],
   "source": [
    "with open('reddit-comments-2015-08.csv', 'r', newline='', encoding='utf-8') as f:\n",
    "    # Initalize a reader object\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    # Skip the header row\n",
    "    next(reader)  \n",
    "    # Split full comments into sentences  - [nltk.sent_tokenize(x[0].lower()) for x in reader] - for the paragraph x[0] from the csv file, make it lowercase and tokenize all sentence\n",
    "    # For all pararaphs in the csv file. * operator unpacks the list into individual sentences, and creates a single iterable\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    # Replace all sentence x in sentences with the start token, sentence body, and text token\"\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    print(sentences[1:10])\n",
    "print (f\"Parsed {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f24abb-60b6-4a98-9c12-8a5baab8dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb815af-c2b8-485f-a4ce-ce59e483fff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SENTENCE_START', 'it', \"'s\", 'a', 'slight', 'ppr', 'league-', '.2', 'ppr', '.', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4cdcd2-50ad-45e3-a475-c09ce4a71011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63023 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print (f\"Found { len(word_freq.items()) } unique words tokens.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d40b91-af5f-4e31-8201-431605c9f65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:\n",
      "[('SENTENCE_END', 79184), ('.', 67334), ('the', 52419), (',', 52137), ('to', 35576), ('i', 32614), ('a', 31777), ('and', 30055), ('of', 23232), ('you', 22457), ('it', 22353), ('that', 19334), ('is', 18196), ('in', 16944), ('*', 14955), ('for', 12541), (\"n't\", 11784), (\"'s\", 11771), (')', 11409)]\n",
      "UNKNOWN_TOKEN\n",
      "Index to word:\n",
      "['SENTENCE_END', '.', 'the', ',', 'to', 'i', 'a', 'and', 'of']\n",
      "1\n",
      "4999\n",
      "1371\n"
     ]
    }
   ],
   "source": [
    "# Get 7999 most common words\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "print(\"Vocab:\")\n",
    "print(vocab[1:20])\n",
    "index_to_word = [ x[0] for x in vocab ]\n",
    "# unknown_token = \"UNKNOWN_TOKEN\"\n",
    "index_to_word.append(unknown_token)\n",
    "print(index_to_word[-1])\n",
    "print(\"Index to word:\")\n",
    "print(index_to_word[1:10])\n",
    "# index_to_word is a list of 8000 words ['word1', 'word2']\n",
    "# enumerate is an object that generates index value pairs in that order\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "# Input a word, it goes into a dictionary, that gets translated to the index of the dictionary of index_to_word\n",
    "# This allows us to represent words as numbers\n",
    "print(word_to_index[\"SENTENCE_END\"])\n",
    "print(word_to_index[\"UNKNOWN_TOKEN\"])\n",
    "print(word_to_index[\"apple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77647de6-5a00-4815-b02a-333d92a31e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SENTENCE_START', 'it', \"'s\", 'a', 'slight', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', '.', 'SENTENCE_END'], ['SENTENCE_START', 'standard', 'besides', '1', 'points', 'for', '15', 'yards', 'receiving', ',', 'UNKNOWN_TOKEN', 'points', 'per', 'UNKNOWN_TOKEN', ',', '6', 'points', 'per', 'UNKNOWN_TOKEN', 'thrown', ',', 'and', 'some', 'bonuses', 'for', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', '.', 'SENTENCE_END'], ['SENTENCE_START', 'my', 'question', 'is', ',', 'is', 'it', 'UNKNOWN_TOKEN', 'clear', 'that', 'qb', 'has', 'the', 'highest', 'potential', 'for', 'points', '?', 'SENTENCE_END'], ['SENTENCE_START', 'i', 'put', 'in', 'the', 'rules', 'at', 'a', 'ranking', 'site', 'and', 'noticed', 'that', 'top', 'qbs', 'had', '300', 'points', 'more', 'than', 'the', 'top', 'UNKNOWN_TOKEN', '.', 'SENTENCE_END'], ['SENTENCE_START', 'would', 'it', 'be', 'dumb', 'not', 'to', 'grab', 'a', 'qb', 'in', 'the', 'first', 'round', '?', 'SENTENCE_END'], ['SENTENCE_START', 'in', 'your', 'scenario', ',', 'a', 'person', 'could', 'just', 'not', 'run', 'the', 'mandatory', 'background', 'check', 'on', 'the', 'buyer', 'and', 'still', 'sell', 'the', 'gun', 'to', 'the', 'felon', '.', 'SENTENCE_END'], ['SENTENCE_START', 'there', \"'s\", 'no', 'way', 'to', 'enforce', 'it', '.', 'SENTENCE_END'], ['SENTENCE_START', 'an', 'honest', 'seller', 'is', 'going', 'to', 'not', 'sell', 'the', 'gun', 'to', 'them', 'when', 'they', 'see', 'they', \"'re\", 'a', 'felon', 'on', 'the', 'background', 'check', '.', 'SENTENCE_END'], ['SENTENCE_START', 'a', 'UNKNOWN_TOKEN', 'seller', 'is', \"n't\", 'going', 'to', 'run', 'the', 'check', 'in', 'the', 'first', 'place', '.', 'SENTENCE_END'], ['SENTENCE_START', 'no', 'one', 'is', 'going', 'to', 'be', 'honest', 'enough', 'to', 'run', 'the', 'check', ',', 'see', 'they', \"'re\", 'a', 'felon', ',', 'and', 'then', 'all', 'of', 'a', 'sudden', 'immediately', 'turn', 'UNKNOWN_TOKEN', 'and', 'say', '``', 'nah', ',', 'you', 'know', 'what', ',', 'here', \"'s\", 'your', 'gun', 'anyway', '.', \"''\", 'SENTENCE_END'], ['SENTENCE_START', 'they', 'would', \"n't\", 'run', 'the', 'fucking', 'check', 'in', 'the', 'first', 'place', ',', 'genius', '.', 'SENTENCE_END'], ['SENTENCE_START', 'your', 'bullshit', 'UNKNOWN_TOKEN', 'is', '*', '*', 'not', 'UNKNOWN_TOKEN', '*', '*', '.', 'SENTENCE_END'], ['SENTENCE_START', 'this', 'is', 'why', 'people', 'without', '&', 'gt', ';', 'here', \"'s\", 'an', 'idea', ',', 'why', 'not', 'make', 'a', 'background', 'check', 'system', 'where', 'it', 'would', 'be', 'illegal', 'to', 'sell', 'guns', 'to', 'UNKNOWN_TOKEN', '?', 'SENTENCE_END'], ['SENTENCE_START', 'that', 'does', \"n't\", 'convince', 'you', '?', 'SENTENCE_END'], ['SENTENCE_START', 'UNKNOWN_TOKEN', '.', 'SENTENCE_END'], ['SENTENCE_START', 'we', 'already', 'fucking', 'have', 'that', '.', 'SENTENCE_END'], ['SENTENCE_START', 'what', 'are', \"n't\", 'you', 'understanding', 'about', 'this', '?', '!', 'SENTENCE_END'], ['SENTENCE_START', 'it', \"'s\", 'just', 'currently', 'not', 'available', 'to', 'private', 'UNKNOWN_TOKEN', '.', 'SENTENCE_END'], ['SENTENCE_START', 'i', \"'d\", 'like', 'to', 'make', 'it', 'available', ',', 'but', 'there', \"'s\", 'no', 'point', 'in', 'making', 'it', 'mandatory', '.', 'SENTENCE_END']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    # Change all words not in word_to_index to unknown_token\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print(tokenized_sentences[1:20])\n",
    "word_to_index[\"it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "852b944c-889d-4122-aeb8-f2f27da82c41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists to store X_train and y_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Iterate over tokenized_sentences\n",
    "for sent in tokenized_sentences:\n",
    "    X_row = []\n",
    "    y_row = []\n",
    "    \n",
    "    # Iterate over words in the sentence\n",
    "    for w in sent[:-1]:\n",
    "        X_row.append(word_to_index.get(w, 0))\n",
    "    \n",
    "    for w in sent[1:]:\n",
    "        y_row.append(word_to_index.get(w, 0))\n",
    "    \n",
    "    X_train.append(X_row)\n",
    "    y_train.append(y_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67dbad77-d2bd-4b79-8d86-b4d8a60a289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 52, 28, 17, 10, 858, 55, 26, 35, 70]\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[52, 28, 17, 10, 858, 55, 26, 35, 70, 1]\n",
      "[[0, 11, 18, 7, 3030, 4999, 4999, 4999, 4999, 2], [0, 981, 1496, 221, 600, 16, 773, 3414, 2967, 4, 4999, 600, 471, 4999, 4, 435, 600, 471, 4999, 2722, 4, 8, 72, 4959, 16, 4999, 4999, 2]]\n",
      "[[11, 18, 7, 3030, 4999, 4999, 4999, 4999, 2, 1], [981, 1496, 221, 600, 16, 773, 3414, 2967, 4, 4999, 600, 471, 4999, 4, 435, 600, 471, 4999, 2722, 4, 8, 72, 4959, 16, 4999, 4999, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print(f\"x:\\n{' '.join([index_to_word[x] for x in x_example])}\\n{x_example}\")\n",
    "print(f\"y:\\n{' '.join([index_to_word[x] for x in y_example])}\\n{y_example}\")\n",
    "\n",
    "print(X_train[1:3])\n",
    "print(y_train[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdded533-603b-4893-bbc9-6bc8dd643a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        # Initalizing weights based on research\n",
    "        # hidden size x vocab size\n",
    "        self.U = np.random.uniform ( -np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        # vocab size x hidden size\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        # hidden size x hidden size\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # *** We are indxing U by x[t]. This is the same as multiplying U with a one-hot vector. The rest of the entries are zero\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "                dLdU[:,x[bptt_step]] += delta_t\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "\n",
    "    def numpy_sgd_step(self, x, y, learning_rate):\n",
    "        # Calculate the gradients\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        # Change parameters according to gradients and learning rate\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "\n",
    "    def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        # We keep track of the losses so we can plot them later\n",
    "        losses = []\n",
    "        num_examples_seen = 0\n",
    "        for epoch in range(nepoch):\n",
    "            # Optionally evaluate the loss\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = model.calculate_loss(X_train, y_train)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                    learning_rate = learning_rate * 0.5  \n",
    "                    print (\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            # For each training example...\n",
    "            for i in range(len(y_train)):\n",
    "                # One SGD step\n",
    "                model.numpy_sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a937bef3-1b62-4934-bd26-96ae813840d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of vector x\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A 1-D numpy array\n",
    "    \n",
    "    Returns:\n",
    "    s -- Softmax of x\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))  # Subtracting the maximum value for numerical stability\n",
    "    s = e_x / e_x.sum(axis=0)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a164d151-d842-457b-ac6f-26177ad924f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 5000)\n",
      "[[0.00020061 0.00019874 0.00019952 ... 0.00019839 0.00020138 0.00020065]\n",
      " [0.00020051 0.00019942 0.00019986 ... 0.00020058 0.00019873 0.00019918]\n",
      " [0.0001985  0.00020077 0.00020189 ... 0.00019998 0.00019918 0.00020052]\n",
      " ...\n",
      " [0.00020098 0.00020034 0.00019928 ... 0.00020072 0.00020173 0.00020064]\n",
      " [0.00019828 0.00020135 0.00020006 ... 0.00019925 0.00020087 0.00019965]\n",
      " [0.00020064 0.0001999  0.00020018 ... 0.00019732 0.00020039 0.0001984 ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print (o.shape)\n",
    "print (o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed3aa64c-65fb-4322-84a9-6c37b0ccb1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print (predictions.shape)\n",
    "print (len(predictions))\n",
    "print (len(y_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bea50efc-ad15-49bf-abbc-0060c0a34042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.517193\n",
      "Actual loss: 8.517153\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print (\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print (\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80cb6950-751d-4537-858b-7a6eec0da2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.9 ms ± 2.94 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.numpy_sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8dafed-29fc-44e5-a01c-26c1674974e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-08 22:48:10: Loss after num_examples_seen=0 epoch=0: 8.517013\n",
      "2024-05-08 22:48:14: Loss after num_examples_seen=100 epoch=1: 8.504391\n",
      "2024-05-08 22:48:18: Loss after num_examples_seen=200 epoch=2: 8.480773\n",
      "2024-05-08 22:48:21: Loss after num_examples_seen=300 epoch=3: 6.621086\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "model.train_with_sgd(X_train[:100], y_train[:100], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "06b96a46-3119-4a17-b587-2cd5a9a7e007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was first everyone job job job job job job job job\n",
      "was first everyone job job job job job job job job\n",
      "was first everyone job job job job job job job job\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, max_iterations=12):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    iterations = 0\n",
    "    # Repeat until we get an end token or reach the maximum number of iterations\n",
    "    while (not new_sentence[-1] == word_to_index[sentence_end_token]) and (iterations < max_iterations):\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        # Get the index of the word with the highest probability\n",
    "        sampled_word = np.argmax(next_word_probs[-1])\n",
    "        # If the sampled word is unknown, continue to the next iteration\n",
    "        if sampled_word == word_to_index[unknown_token]:\n",
    "            continue\n",
    "        new_sentence.append(sampled_word)\n",
    "        iterations += 1\n",
    "    # Convert indices to words\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    "\n",
    "num_sentences = 3\n",
    "senten_min_length = 7\n",
    "\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print (\" \".join(sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
