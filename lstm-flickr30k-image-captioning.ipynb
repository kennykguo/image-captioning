{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4845244,"sourceType":"datasetVersion","datasetId":2808179}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport string\nfrom collections import Counter, defaultdict\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils as nn_utils\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import InterpolationMode\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torchvision import models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define paths\nimages_path = '/kaggle/input/d/eeshawn/flickr30k/flickr30k_images'\ncaptions_file = '/kaggle/input/d/eeshawn/flickr30k/captions.txt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creates a dictionary, with the image name as the key, and the caption as the value\ncaptions = defaultdict(list)\nwith open(captions_file, 'r') as f:\n    next(f)  # Skip the header\n    for line in f:\n        image, caption = line.strip().split(',', 1)\n        captions[image].append(caption)\n\n# Example caption\ncaptions['1000092795.jpg']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess captions and clean the text up\ndef clean_caption(caption):\n    # Remove punctuation\n    caption = caption.translate(str.maketrans('', '', string.punctuation))\n    # Remove words with numbers and words of length 1\n    # If the word is of length 1, or the word contains a digit, remove it from the cpation\n    caption = ' '.join([word for word in caption.split() if len(word) > 1 and not any(char.isdigit() for char in word)])\n    # Convert to lowercase\n    caption = caption.lower()\n    return caption\n\n#Cleans all captions in captions dictionary\nfor image in captions:\n    # Loops over all strings, and cleans the captions\n    captions[image] = [clean_caption(caption) for caption in captions[image]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define image transformation - with augmentations\n# Resize((256, 256)): Resizes the image to a slightly larger size than the target.\n# RandomCrop((224, 224)): Randomly crops the image to the target size, introducing positional variance.\n# RandomHorizontalFlip(p=0.5): Flips the image horizontally with a 50% probability.\n# ColorJitter: Randomly changes brightness, contrast, saturation, and hue.\n# RandomRotation: Randomly rotates the image by up to 10 degrees.\n# RandomAffine: Applies random affine transformations (translation and scaling).\n# ToTensor(): Converts the image to a tensor.\n# Normalize: Normalizes the image using ImageNet mean and standard deviation.\n# Lambda: Ensures the image has 3 channels (as in your original pipeline).\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),  # Resize to slightly larger than target size\n    transforms.RandomCrop((224, 224)),  # Random crop to target size\n    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip with 50% probability\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jittering\n    transforms.RandomRotation(degrees=10, interpolation=InterpolationMode.BILINEAR),  # Random rotation\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Random affine transformation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize to ImageNet stats\n    transforms.Lambda(lambda x: x[:3, :, :]),  # Ensure the image has 3 channels\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.Lambda(lambda x: x[:3, :, :]),\n])\n\n# Load image function helper\ndef load_image(image_file):\n    image = Image.open(os.path.join(images_path, image_file)).convert('RGB')\n    image = transform(image)\n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build vocabulary count to omit rare words\nword_counter = Counter()\nfor image, caption_list in captions.items():\n    # Loops through list of strings\n    for caption in caption_list:\n        word_counter.update(caption.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dictionaries for words and include words that only appear a number of times above the threshold\nvocab_threshold = 3\nvocab = [word for word, count in word_counter.items() if count > vocab_threshold]\nvocab_size = len(vocab) + 4  # Including <PAD>, <START>, <END>, <UNK>","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dictionary mapping stoi and itos\nstoi = {word: idx + 4 for idx, word in enumerate(vocab)}\nstoi['<PAD>'] = 0\nstoi['<START>'] = 1\nstoi['<END>'] = 2\nstoi['<UNK>'] = 3\nitos = {idx: word for word, idx in stoi.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper functions\ndef encode_caption(caption):\n    # Default entry for dictionary.get()\n    encoded = [stoi['<START>']] + [stoi.get(word, stoi['<UNK>']) for word in caption.split()] + [stoi['<END>']]\n    return encoded\n\ndef decode_caption(encoded_caption):\n    # Default entry for dictionary.get()\n    return ' '.join([itos.get(idx, '<UNK>') for idx in encoded_caption])\n\ndef pad_caption(caption, max_length):\n    # This actually pads one less than the max, which is not intended. But, we will keep this anyways\n    return caption + [stoi['<PAD>']] * (max_length - len(caption))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the max length of the captions\n# To unpack this, we are looping over the defaultdict captions. We get the list of strings called caption_list, and loop over all lists of strings in captions, and also loop over all captions in the list\n# This is a very complex string comprehension\n# We add 2 to the max_length, so that there will always be space for atleast one END token, and one PAD token\nmax_length = max(len(caption.split()) for caption_list in captions.values() for caption in caption_list) + 2\nmax_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loops over all image keys, all caption lists, all captions in caption lists, encodes them, and pads them\nencoded_captions = defaultdict(list)\n\nfor image in captions:\n    for caption in captions[image]:\n        encoded = encode_caption(caption)\n        padded = pad_caption(encoded, max_length)\n        encoded_captions[image].append(padded)\n        \n# encoded_captions['1000092795.jpg'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create input and target sequences\n# Here, we are cutting off the input sequence by 1 token, so that the target sequence won't be incorrect\ndef create_sequences(encoded_caption):\n    input_sequence = encoded_caption[:-1]\n    target_sequence = encoded_caption[1:]\n    return input_sequence, target_sequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create input and target sequences for all captions\ninput_sequences = defaultdict(list)\ntarget_sequences = defaultdict(list)\n\n# Looping over all image key, list of captions\nfor image, caption_list in encoded_captions.items():\n    for caption in caption_list:\n        # Loop through all captions in all caption_lists, and create the corresponding token lists\n        input_seq, target_seq = create_sequences(caption)\n        # Create another defaultdict() that allows for indexing of the processed list tokens\n        input_sequences[image].append(input_seq)\n        target_sequences[image].append(target_seq)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating randomized batches\n# List of image keys\nimage_keys = list(captions.keys())\ntorch.manual_seed(1337)\n# Shuffle the iamge keys\nshuffled_indices = torch.randperm(len(image_keys)).tolist()\nimage_keys = [image_keys[i] for i in shuffled_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the keys (80% training, 20% validation)\nsplit_index = int(0.8 * len(image_keys))\ntrain_keys = image_keys[:split_index]\nval_keys = image_keys[split_index:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create train and validation sets\ntrain_captions = {key: captions[key] for key in train_keys}\nval_captions = {key: captions[key] for key in val_keys}\ntrain_input_sequences = {key: input_sequences[key] for key in train_keys}\nval_input_sequences = {key: input_sequences[key] for key in val_keys}\ntrain_target_sequences = {key: target_sequences[key] for key in train_keys}\nval_target_sequences = {key: target_sequences[key] for key in val_keys}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Randomized batch sampling\n# def create_batch(caption_dict, input_seq_dict, target_seq_dict, batch_size):\n#     images_batch = []\n#     input_captions_batch = []\n#     target_captions_batch = []\n    \n#     selected_images = random.sample(list(caption_dict.keys()), batch_size)\n#     for image in selected_images:\n#         # Pick a random index from one of the caption lists\n#         selected_caption_idx = random.randint(0, len(caption_dict[image]) - 1)\n#         # Add the corresponding image caption\n#         input_captions_batch.append(input_seq_dict[image][selected_caption_idx])\n#         # Get the corresponding target_caption\n#         target_captions_batch.append(target_seq_dict[image][selected_caption_idx])\n#         # Add the image to the corresponding image to the image set\n#         images_batch.append(load_image(image))\n    \n#     images_batch = torch.stack(images_batch)\n#     input_captions_batch = torch.tensor(input_captions_batch)\n#     target_captions_batch = torch.tensor(target_captions_batch)\n    \n#     return images_batch, input_captions_batch, target_captions_batch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Non-randomzed batching, indexed by image_keys\ndef create_batch(caption_dict, input_seq_dict, target_seq_dict, batch_size, image_keys):\n    images_batch = []\n    input_captions_batch = []\n    target_captions_batch = []\n    \n    # Loop over all image keys\n    for image in image_keys[:batch_size]:\n        # Pick a random caption for the image\n        selected_caption_idx = random.randint(0, len(caption_dict[image]) - 1)\n        input_captions_batch.append(input_seq_dict[image][selected_caption_idx])\n        target_captions_batch.append(target_seq_dict[image][selected_caption_idx])\n        images_batch.append(load_image(image))\n    \n    # Convert to expected shapes\n    images_batch = torch.stack(images_batch)\n    input_captions_batch = torch.tensor(input_captions_batch)\n    target_captions_batch = torch.tensor(target_captions_batch)\n    \n    return images_batch, input_captions_batch, target_captions_batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage of function\nbatch_size = 4\nbatch_keys = train_keys[0:0+batch_size]\nimage_batch, caption_batch, label_batch = create_batch(train_captions, train_input_sequences, train_target_sequences, len(batch_keys), batch_keys)\nprint(image_batch.shape)\nprint(caption_batch.shape)\nprint(label_batch.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_example(caption_dict, input_seq_dict, target_seq_dict):\n    \n    # Select a random image\n    random_image = random.choice(list(caption_dict.keys()))\n    \n    # Select a random caption index for the chosen image\n    caption_idx = random.randint(0, len(caption_dict[random_image]) - 1)\n    \n    # Load the image\n    image_tensor = load_image(random_image)\n    image_array = image_tensor.permute(1, 2, 0).numpy()\n    \n    # Get the corresponding caption inputs and targets\n    input_caption_encoded = input_seq_dict[random_image][caption_idx]\n    target_caption_encoded = target_seq_dict[random_image][caption_idx]\n    input_caption = decode_caption(input_caption_encoded)\n    target_caption = decode_caption(target_caption_encoded)\n    \n    # Plot the image\n    plt.imshow(image_array)\n    plt.axis('off')\n    plt.title('Image')\n    plt.show()\n    \n    # Print the input and target captions\n    print(f\"Input Caption: {input_caption}\")\n    print(f\"Target Caption: {target_caption}\")\n\n# Visualize one of the examples\nprint(\"Training example:\")\nvisualize_example(train_captions, train_input_sequences, train_target_sequences)\n\nprint(\"Validation example:\")\nvisualize_example(val_captions, val_input_sequences, val_target_sequences)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a residual block for MLP architecture\nclass ResBlockMLP(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(ResBlockMLP, self).__init__()\n        \n        self.norm1 = nn.LayerNorm(input_size)\n        self.fc1 = nn.Linear(input_size, input_size//2)\n        self.norm2 = nn.LayerNorm(input_size//2)\n        self.fc2 = nn.Linear(input_size//2, output_size)\n        self.fc3 = nn.Linear(input_size, output_size)\n        self.act = nn.ELU()\n\n    def forward(self, x):\n        x = self.act(self.norm1(x))\n        skip = self.fc3(x)\n        x = self.act(self.norm2(self.fc1(x)))\n        x = self.fc2(x)\n        return x + skip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvNet(nn.Module):\n    def __init__(self, dropout):\n        super(ConvNet, self).__init__()\n        \n        # Load the pretrained ResNet50 model\n        self.resnet50 = models.resnet50(pretrained=True)\n        \n        # Freeze all the ResNet50 layers except the last fully connected layer\n        for param in self.resnet50.parameters():\n            param.requires_grad = False\n        for param in self.resnet50.fc.parameters():\n            param.requires_grad = True\n        \n        # Add dropout before the final fully connected layer\n        self.resnet50.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            self.resnet50.fc\n        )\n    \n    def forward(self, inputs):  # (224, 224, 3)\n        out = self.resnet50(inputs)\n        return out  # (B, 1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, channels_in, dropout, vocab_size, emb_size, hidden_size, num_layers, num_blocks):\n        super(Decoder, self).__init__()\n        \n        self.convnet = ConvNet(dropout = dropout)\n\n        self.linear = nn.Linear(1000, hidden_size)\n        \n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        \n        # Define LSTM layer\n        self.lstm = nn.LSTM(input_size = emb_size, hidden_size = hidden_size, num_layers = num_layers, bias = False, batch_first = True)\n\n        # Residual blocks\n        self.blocks = nn.Sequential(*[ResBlockMLP(hidden_size, hidden_size) for _ in range(num_blocks)])\n\n        # Activation function\n        self.act = nn.ELU()\n        \n        # Define the output fully connected layer\n        self.fc_out = nn.Linear(hidden_size, vocab_size)\n\n    # Memory in will always be a matrice of zeros\n    def forward(self, images, captions, mem_in):\n\n        features = self.convnet(images) # (B, 4096)\n        features = self.linear(features) # (B, hidden_size)\n        features = features.unsqueeze(0).repeat(self.lstm.num_layers, 1, 1) # (num_layers, B, hidden_size)\n\n        batch_size = images.size(0)\n        hidden_size = self.lstm.hidden_size\n        num_layers = self.lstm.num_layers\n\n        captions = self.embedding(captions)\n        \n        outputs, (hidden_out, mem_out) = self.lstm(captions, (features, mem_in))\n        # x = (B, T, emb_size)\n        # hidden_out = (num_layers, B, hidden_size)\n        # mem_out = (num_layers, B, hidden_size)\n\n        outputs = self.act(self.blocks(outputs))\n        \n        return self.fc_out(outputs) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UPDATE the model parameters here\n\ndevice = torch.device(0 if torch.cuda.is_available() else 'cpu')\n\nnum_iterations = 1000\n\nnepochs = 10\n\nlearning_rate = 1e-4\n\nchannels_in = 3\n\ndropout = 0.4\n\nvocab_size = len(stoi)\n\nemb_size = 2048\n\nhidden_size = 2048\n\n# LSTM layers\nnum_layers = 2\n\n# ResBlocks\nnum_blocks = 2\n\nbatch_size = 128\n\nmax_length = max_length\n\n# Paddding 1 less, than the max_length\ntime_steps = max_length - 1\n\nmax_grad_norm = 5.0  # Gradient clipping threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Decoder(channels_in = channels_in, dropout = dropout, vocab_size = vocab_size, emb_size = emb_size, hidden_size = hidden_size, num_layers = num_layers, num_blocks = num_blocks).to(device)\n\nmemory = torch.zeros(num_layers, batch_size, hidden_size, device=device)\n\noptimizer = optim.AdamW([\n    {'params': model.linear.parameters()},\n    {'params': model.embedding.parameters()},\n    {'params': model.lstm.parameters()},\n    {'params': model.blocks.parameters()},\n    {'params': model.fc_out.parameters()}\n], lr=learning_rate, weight_decay=1e-4)\n\nlr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs, eta_min=0)\n\nloss_fn = nn.CrossEntropyLoss()\n\ntraining_loss = []\n\nval_loss = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate number of model parameters\nnum_model_params = 0\nfor param in model.parameters():\n    num_model_params += param.flatten().shape[0]\n\nprint(\"This model Has %d Parameters, %d Million\" % (num_model_params, num_model_params//1e6 ) )\n\n# 154M\n# 163M","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def calculate_average_val_loss(model, val_captions, val_input_sequences, val_target_sequences, batch_size, loss_fn, num_examples=100):\n#     model.eval()\n#     total_loss = 0\n#     num_batches = 0\n#     examples_processed = 0\n    \n#     with torch.no_grad():\n#         while examples_processed < num_examples:\n#             image_batch, caption_batch, label_batch = create_batch(val_captions, val_input_sequences, val_target_sequences, batch_size)\n#             current_batch_size = image_batch.size(0)\n#             image_batch = image_batch.to(device)\n#             caption_batch = caption_batch.to(device)\n#             label_batch = label_batch.to(device)\n            \n#             outputs = model(image_batch, caption_batch, memory)\n#             outputs = outputs.reshape(current_batch_size * time_steps, -1)\n#             label_batch = label_batch.reshape(current_batch_size * time_steps)\n            \n#             loss = loss_fn(outputs, label_batch)\n#             total_loss += loss.item() * current_batch_size\n#             examples_processed += current_batch_size\n#             num_batches += 1\n    \n#     average_loss = total_loss / examples_processed\n#     return average_loss\n\n# # average_val_loss = calculate_average_val_loss(model, val_captions, val_input_sequences, val_target_sequences, batch_size, loss_fn)\n# # print(\"iteration\", iteration, \"average val loss:\", average_val_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_average_val_loss(model, val_captions, val_input_sequences, val_target_sequences, batch_size, loss_fn):\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    # Calculate the number of complete batches\n    num_complete_batches = len(val_keys) // batch_size\n    \n    with torch.no_grad():\n        for i in range(0, num_complete_batches * batch_size, batch_size):\n            batch_keys = val_keys[i:i+batch_size]\n            image_batch, caption_batch, label_batch = create_batch(val_captions, val_input_sequences, val_target_sequences, len(batch_keys), batch_keys)\n            \n            image_batch = image_batch.to(device)\n            caption_batch = caption_batch.to(device)\n            label_batch = label_batch.to(device)\n            \n            outputs = model(image_batch, caption_batch, memory)\n            outputs = outputs.reshape(-1, vocab_size)\n            label_batch = label_batch.reshape(-1)\n            \n            loss = loss_fn(outputs, label_batch)\n            total_loss += loss.item()\n            num_batches += 1\n    \n    average_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n    return average_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Training loop\n# for iteration in range(num_iterations):\n\n#     # Training\n#     model.train()\n\n#     image_batch, caption_batch, label_batch = create_batch(train_captions, train_input_sequences, train_target_sequences, batch_size)\n\n#     image_batch = image_batch.to(device)\n#     caption_batch = caption_batch.to(device)\n#     label_batch = label_batch.to(device)\n    \n#     outputs = model(image_batch, caption_batch, memory)\n\n#     outputs = outputs.reshape(batch_size * time_steps, -1)\n#     label_batch = label_batch.reshape(batch_size * time_steps)\n#     loss = loss_fn(outputs, label_batch)\n#     training_loss.append(loss.item())\n\n#     if iteration % 10 == 0: \n#         print(\"iteration\", iteration, \"training loss:\", loss.item())\n\n#     # Backward pass\n#     optimizer.zero_grad()\n#     loss.backward()\n    \n#     # Clip gradients\n#     nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n#     optimizer.step()\n\n    \n#     # Evaluate every so often\n#     if iteration % 10 == 0:\n        \n#         model.eval()\n\n#         image_batch, caption_batch, label_batch = create_batch(val_captions, val_input_sequences, val_target_sequences, batch_size)\n    \n#         image_batch = image_batch.to(device)\n#         caption_batch = caption_batch.to(device)\n#         label_batch = label_batch.to(device)\n        \n#         outputs = model(image_batch, caption_batch, memory)\n        \n#         outputs = outputs.reshape(batch_size * time_steps, -1)\n    \n#         label_batch = label_batch.reshape(batch_size * time_steps)\n    \n#         loss = loss_fn(outputs, label_batch)\n\n#         val_loss.append(loss.item())\n        \n#         print(\"iteration\", iteration, \"val loss:\", loss.item())\n        \n#     if iteration % 100 == 0:\n#         avg_val_loss = calculate_average_val_loss(model, val_captions, val_input_sequences, val_target_sequences, batch_size, loss_fn)\n#         print(\"------\")\n#         print(\"iteration\", iteration, \"avg val loss:\", avg_val_loss)\n#         print(\"------\")\n\n#     lr_scheduler.step()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(nepochs):\n    \n    print(\"training:\")\n    \n    # Shuffle the dataset at the beginning of each epoch (to ensure that for every epoch, the batches created are randomized)\n    # We train over epochs to ensure that all data in the training_set is trained over\n    random.shuffle(train_keys)\n    \n    # Training\n    model.train()\n    epoch_loss = 0\n    num_batches = 0\n\n    # This loop ensures that we always get a batch of batch_size number of elements, and iterate over all examples generated in the batch\n    # In every iteration, we augment the image keys to reduce overfitting\n    # Calculate the number of complete batches\n    num_complete_batches = len(train_keys) // batch_size\n    \n    for i in range(0, num_complete_batches * batch_size, batch_size):\n        batch_keys = train_keys[i:i+batch_size]\n        image_batch, caption_batch, label_batch = create_batch(train_captions, train_input_sequences, train_target_sequences, len(batch_keys), batch_keys)\n\n        image_batch = image_batch.to(device)\n        caption_batch = caption_batch.to(device)\n        label_batch = label_batch.to(device)\n        \n        outputs = model(image_batch, caption_batch, memory)\n\n        outputs = outputs.reshape(-1, vocab_size)\n        label_batch = label_batch.reshape(-1)\n        loss = loss_fn(outputs, label_batch)\n        epoch_loss += loss.item()\n        num_batches += 1\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Clip gradients\n        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        optimizer.step()\n\n    # Calculate average training loss for the epoch\n    avg_train_loss = epoch_loss / num_batches\n    training_loss.append(avg_train_loss)\n\n    # Validation\n    model.eval()\n    \n    print(\"evaluating:\")\n    \n    val_epoch_loss = calculate_average_val_loss(model, val_captions, val_input_sequences, val_target_sequences, batch_size, loss_fn)\n    val_loss.append(val_epoch_loss)\n\n    # Print epoch results\n    print(f\"Epoch {epoch+1}/{nepochs}, AVG Train Loss: {avg_train_loss:.4f}, AVG Val Loss: {val_epoch_loss:.4f}\")\n\n    # Update learning rate\n    lr_scheduler.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'lstm-flickr30k-image-captioning.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the path to the saved model file\nmodel_path = 'models/image-captioning.pth'\n\n# Load the entire model (architecture + trained parameters)\nmodel = torch.load(model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,5))\ntrain_x = np.linspace(0, num_epochs, len(training_loss))\nplt.plot(train_x, training_loss)\nplt.title(\"Training loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,5))\ntrain_x = np.linspace(0, num_epochs, len(val_loss))\nplt.plot(train_x, val_loss)\nplt.title(\"Val loss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_batch.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to register hooks for saving activations\ndef register_hooks(layers):\n    for layer in layers:\n        if isinstance(layer, (nn.Tanh, nn.ReLU, nn.ELU)):\n            layer.register_forward_hook(lambda self, input, output: setattr(self, 'out', output))\n\n# Register hooks for the model\nregister_hooks(list(model.modules()))\n\n# Dummy forward pass\noutputs = model(image_batch.to(device), caption_batch.to(device), memory.to(device))\n\n# Visualize activations after a training iteration\ndef visualize_activations(model):\n    plt.figure(figsize=(20, 4))\n    print(\"Activations:\")\n    legends = []\n    for i, layer in enumerate(model.modules()):\n        if hasattr(layer, 'out'):\n            t = layer.out\n            print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean().item(), t.std().item(), (t.abs() > 0.97).float().mean().item()*100))\n            hy, hx = torch.histc(t, bins=50, min=t.min().item(), max=t.max().item()).cpu().numpy(), torch.linspace(t.min().item(), t.max().item(), steps=50).cpu().numpy()\n            plt.plot(hx, hy)\n            legends.append(f'layer {i} ({layer.__class__.__name__})')\n    plt.legend(legends)\n    plt.title('Activation Distribution')\n    plt.show()\n\n# Visualize gradients after a training iteration\ndef visualize_gradients(model):\n    plt.figure(figsize=(20, 4))\n    print(\"Gradients:\")\n    legends = []\n    for i, layer in enumerate(model.modules()):\n        if hasattr(layer, 'weight') and layer.weight.grad is not None:\n            t = layer.weight.grad\n            print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean().item(), t.std().item()))\n            hy, hx = torch.histc(t, bins=50, min=t.min().item(), max=t.max().item()).cpu().numpy(), torch.linspace(t.min().item(), t.max().item(), steps=50).cpu().numpy()\n            plt.plot(hx, hy)\n            legends.append(f'layer {i} ({layer.__class__.__name__})')\n    plt.legend(legends)\n    plt.title('Gradient Distribution')\n    plt.show()\n\n# Visualize weights gradient distribution after a training iteration\ndef visualize_weights_gradient(model):\n    plt.figure(figsize=(20, 4))\n    print(\"Weight gradients:\")\n    legends = []\n    for i, p in enumerate(model.parameters()):\n        if p.grad is not None:\n            t = p.grad\n            if p.ndim == 2:\n                print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean().item(), t.std().item(), t.std().item() / p.std().item()))\n                hy, hx = torch.histc(t, bins=50, min=t.min().item(), max=t.max().item()).cpu().numpy(), torch.linspace(t.min().item(), t.max().item(), steps=50).cpu().numpy()\n                plt.plot(hx, hy)\n                legends.append(f'{i} {tuple(p.shape)}')\n    plt.legend(legends)\n    plt.title('Weights Gradient Distribution')\n    plt.show()\n\n# After a training iteration, call these functions to visualize the statistics\nvisualize_activations(model)\nvisualize_gradients(model)\nvisualize_weights_gradient(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sampling functions with BLEU scores\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef sample_caption(image, max_length=30):\n    model.eval()\n    \n    sos_token = torch.tensor([[stoi['<START>']]], device=device)\n    \n    features = model.convnet(image.unsqueeze(0).to(device))  # Get features from the image\n    features = model.linear(features)  # Transform features to hidden size\n    features = features.unsqueeze(0).repeat(num_layers, 1, 1)  # Add time dimension (1, 1, hidden_size)\n    \n    input_tokens = sos_token  # Start with <sos> token\n    generated_caption = []\n    \n    for _ in range(max_length):\n        input_embed = model.embedding(input_tokens)  # Embed the input tokens\n        \n        output, _ = model.lstm(input_embed, (features, torch.zeros_like(features)))\n        \n        output = model.blocks(output)\n        \n        output = model.fc_out(output)  # Output shape: (1, seq_length, vocab_size)\n        \n        probabilities = F.softmax(output[:, -1, :], dim=-1)\n        \n#         next_token = torch.argmax(probabilities, dim=-1).unsqueeze(0)\n        next_token = torch.multinomial(probabilities, num_samples=1)\n        \n        generated_caption.append(itos[next_token.item()])\n        \n        input_tokens = torch.cat([input_tokens, next_token], dim=1)\n\n        if next_token.item() == stoi['<END>'] or next_token.item() == stoi['<PAD>']:\n            break\n    \n    return generated_caption\n\ndef visualize_example(caption_dict, input_seq_dict, target_seq_dict, max_length=30):\n    random_image = random.choice(list(caption_dict.keys()))\n    caption_idx = random.randint(0, len(caption_dict[random_image]) - 1)\n    \n    image_tensor = load_image(random_image)\n    image_array = image_tensor.permute(1, 2, 0).numpy()\n    \n    plt.imshow(image_array)\n    plt.axis('off')\n    plt.title('Image')\n    plt.show()\n    \n    original_caption_encoded = input_seq_dict[random_image][caption_idx]\n    original_caption = decode_caption(original_caption_encoded)\n    print(f\"Original Caption: {original_caption}\")\n    \n    references = [original_caption.split()]  # Tokenized reference caption\n    smoothing_function = SmoothingFunction().method1\n    \n    for _ in range(5):\n        generated_caption = sample_caption(image_tensor, max_length)\n        print(f\"Generated Caption: {' '.join(generated_caption)}\")\n        \n        # Calculate BLEU-1 and BLEU-4 scores\n        bleu_1 = sentence_bleu(references, generated_caption, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n        bleu_4 = sentence_bleu(references, generated_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n        \n        print(f\"BLEU-1 Score: {bleu_1:.4f}\")\n        print(f\"BLEU-4 Score: {bleu_4:.4f}\")\n\n# Example usage\nnum_samples = 5\nfor _ in range(num_samples):\n    visualize_example(val_captions, val_input_sequences, val_target_sequences, max_length=max_length)\nprint(\"----\")\nfor _ in range(num_samples):\n    visualize_example(train_captions, train_input_sequences, train_target_sequences, max_length=max_length)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Beam search implementation\nimport heapq\n\ndef beam_search_caption(model, image, beam_width=10, max_length=30):\n    model.eval()\n    \n    sos_token = torch.tensor([[stoi['<START>']]], device=device)\n    \n    features = model.convnet(image.unsqueeze(0).to(device))  # Get features from the image\n    features = model.linear(features)  # Transform features to hidden size\n    features = features.unsqueeze(0).repeat(num_layers, 1, 1)  # Add time dimension (1, 1, hidden_size)\n    \n    # Initialize the beam\n    beam = [(0, sos_token, features, torch.zeros_like(features), [])]\n    \n    for _ in range(max_length):\n        candidates = []\n        \n        for cumulative_score, tokens, h, c, caption in beam:\n            if caption and caption[-1] in [stoi['<END>'], stoi['<PAD>']]:\n                candidates.append((cumulative_score, tokens, h, c, caption))\n                continue\n            \n            input_embed = model.embedding(tokens)\n            output, (h_n, c_n) = model.lstm(input_embed, (h, c))\n            output = model.blocks(output)\n            output = model.fc_out(output)\n            \n            probabilities = F.log_softmax(output[:, -1, :], dim=-1)\n            top_scores, top_indices = probabilities.topk(beam_width)\n            \n            for score, idx in zip(top_scores.squeeze(), top_indices.squeeze()):\n                new_tokens = torch.cat([tokens, idx.unsqueeze(0).unsqueeze(0)], dim=1)\n                new_caption = caption + [idx.item()]\n                new_cumulative_score = cumulative_score + score.item()\n                candidates.append((new_cumulative_score, new_tokens, h_n, c_n, new_caption))\n        \n        # Select top beam_width candidates\n        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n        \n        # Check if all beams have ended\n        if all(caption[-1] in [stoi['<END>'], stoi['<PAD>']] for _, _, _, _, caption in beam):\n            break\n    \n    # Return the caption with the highest score\n    best_caption = max(beam, key=lambda x: x[0])[4]\n    return [itos[token] for token in best_caption]\n\n# Modify visualize_example to use beam_search_caption\ndef visualize_example(caption_dict, input_seq_dict, target_seq_dict, max_length=30):\n    random_image = random.choice(list(caption_dict.keys()))\n    caption_idx = random.randint(0, len(caption_dict[random_image]) - 1)\n    \n    image_tensor = load_image(random_image)\n    image_array = image_tensor.permute(1, 2, 0).numpy()\n    \n    plt.imshow(image_array)\n    plt.axis('off')\n    plt.title('Image')\n    plt.show()\n    \n    original_caption_encoded = input_seq_dict[random_image][caption_idx]\n    original_caption = decode_caption(original_caption_encoded)\n    print(f\"Original Caption: {original_caption}\")\n    \n    references = [original_caption.split()]  # Tokenized reference caption\n    smoothing_function = SmoothingFunction().method1\n    \n    generated_caption = beam_search_caption(model, image_tensor, beam_width=10, max_length=max_length)\n    print(f\"Generated Caption: {' '.join(generated_caption)}\")\n    \n    # Calculate BLEU-1 and BLEU-4 scores\n    bleu_1 = sentence_bleu(references, generated_caption, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n    bleu_4 = sentence_bleu(references, generated_caption, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n    \n    print(f\"BLEU-1 Score: {bleu_1:.4f}\")\n    print(f\"BLEU-4 Score: {bleu_4:.4f}\")\n\n# Example usage remains the same\nnum_samples = 5\nfor _ in range(num_samples):\n    visualize_example(val_captions, val_input_sequences, val_target_sequences, max_length=max_length)\nprint(\"----\")\nfor _ in range(num_samples):\n    visualize_example(train_captions, train_input_sequences, train_target_sequences, max_length=max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}